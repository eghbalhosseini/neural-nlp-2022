import sys
import yaml
import os
import argparse
from pathlib import Path
import torch
from collections import namedtuple
from neural_nlp.models.gpt_neox_model.modeling_gpt_neox import GPTNeoXForCausalLM
from neural_nlp.models.gpt_neox_model.modeling_gpt_neox_learned import GPTNeoXPosLearnedForCausalLM
from neural_nlp.models.gpt_neox_model.configuration_gpt_neox import GPTNeoXConfig , GPTNeoXPosLearnedConfig
from transformers import AutoTokenizer, GPT2Config
from neural_nlp.models.gpt_neox_model.neox_args import NeoXArgsAll
import glob
import json
from tqdm import tqdm

def get_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--checkpoint_dir", type=str, required=True,
                        help="directory that contains state dict pt files and a config directory generated by gpt-neox")
    parser.add_argument("--hf_save_dir", type=str, required=True,
                        help="directory to save Huggingface GPT-NeoX model weights and configuration")
    args = parser.parse_args()
    return args

def mock_get_args():
    mock_args = namedtuple('debug', ['checkpoint_dir', 'hf_save_dir'])
    debug_args = mock_args('/om/user/ehoseini/MyData/miniBERTa_training/miniBERTa_100m_v2/gpt2/checkpoints_0/global_step200/',
                        '/om/user/ehoseini/MyData/miniBERTa_training/miniBERTa_100m_v2/gpt2/checkpoints_0/global_step200/'
                          )
    return debug_args

def convert_gpt_neox_config_to_hf(config_dir):
    print(f"Extracting configs from files in {config_dir}")
    config_path = Path(config_dir, '*.yml')
    config_files = glob.glob(str(config_path))
    if len(config_files) > 0:
        config = {}
        for filename in config_files:
            with open(os.path.join(config_dir, filename)) as f:
                data = f.read()
            yaml_dict = yaml.load(data, Loader=yaml.CLoader)
            config.update(yaml_dict)
    # figure out activation
    p=list(config.keys())
    p=[x.replace('-','_') for x in p]
    d = dict(zip(p, list(config.values())))
    config_Neo = NeoXArgsAll()
    config_Neo.update_values(d)

    if getattr(config_Neo,'activation',None) is not None:
        activation_function = config_Neo.activation
    else:
        if getattr(config_Neo,'bias_gelu_fusion',None):
            activation_function = "gelu_fast"
        elif getattr(config_Neo,'openai_gelu',None):
            activation_function = "gelu_new"
        else:
            activation_function = "gelu"
    # assume a vocab size
    if getattr(config_Neo,'padded_vocab_size',None) is not None:
        vocab_size = config_Neo.padded_vocab_size
    else:
        # assume a location for the vocab file

        vocab_file = Path('/om/user/ehoseini/gpt-neox/data/gpt2-vocab.json')
        with open(str(vocab_file), 'rb')  as vocab:
            vocab_data = vocab.read()
            vocab_size = len(json.loads(vocab_data))
        # if getattr(config_Neo,'make_vocab_size_divisible_by',None) is not None:
        #     q = int(vocab_size / config_Neo.make_vocab_size_divisible_by)
        #     padded_vocab_size = (q + 1) * config_Neo.make_vocab_size_divisible_by
        #     vocab_size = padded_vocab_size
    if config_Neo.pos_emb=='learned':
        config_class=GPTNeoXPosLearnedConfig
    else:
        config_class = GPTNeoXConfig
    config = config_class(
        vocab_size=vocab_size,
        n_positions=config_Neo.max_position_embeddings,
        n_embd=config_Neo.hidden_size,
        n_layer=config_Neo.num_layers,
        n_head=config_Neo.num_attention_heads,
        n_inner=config_Neo.hidden_size * 4,  # this is default
        activation_function=activation_function,
        # resid_pdrop=0.1, not set
        embd_pdrop=config_Neo.hidden_dropout,
        attn_pdrop=config_Neo.attention_dropout,
        layer_norm_epsilon=config_Neo.layernorm_epsilon,
        initializer_range=config_Neo.init_method_std,
        pos_emb=config_Neo.pos_emb,
        norm=config_Neo.norm
        # summary_type="cls_index", not implemented
        #summary_use_proj=True,
        # summary_activation=None, not implemented
        # summary_proj_to_labels=True,
        # summary_first_dropout=0.1,
        # scale_attn_weights=True,
        #use_cache=True,
        #bos_token_id=50256,
        #eos_token_id=50256,
    )
    tokenizer_type = config_Neo.tokenizer_type
    if tokenizer_type == "GPT2BPETokenizer":
        tokenizer_model_name = "gpt2"
    elif tokenizer_type == "PretrainedFromHF":
        tokenizer_model_name = config_Neo.tokenizer_name_or_path
    else:
        raise ValueError(f"Unrecognized tokenizer_type {tokenizer_type}")

    tokenizer = AutoTokenizer.from_pretrained(tokenizer_model_name)
    tokenizer_class = type(tokenizer).__name__
    config.tokenizer_class = tokenizer_class
    #print("Saving config")
    #config.save_pretrained(config_dir)
    return config

def get_state_dict_from_checkpoint_dir(checkpoint_dir, config):
    tgt_state_dict = {}
    num_layers=config.n_layer
    # word embedding
    src_state_dict = torch.load(os.path.join(checkpoint_dir, "layer_00-model_00-model_states.pt"))
    # Truncate the embedding table to vocab_size rows.
    word_embeddings = src_state_dict["word_embeddings.weight"]
    word_embeddings = word_embeddings[: config.vocab_size, :]
    tgt_state_dict["transformer.wte.weight"] = word_embeddings
    if 'position_embeddings.weight' in src_state_dict.keys():
        position_embeddings = src_state_dict["position_embeddings.weight"]
        tgt_state_dict["transformer.wpe.weight"] = position_embeddings

    # layers
    for layer_idx in tqdm(range(1, num_layers + 1),desc=f'reading layers'):
        src_state_dict = torch.load(os.path.join(checkpoint_dir, f"layer_{layer_idx + 1:02}-model_00-model_states.pt"))
        if config.pos_emb=='learned':
            assert (len(src_state_dict.keys()) == 12)  # we are only accounting for 12 keys here
        elif config.pos_emb=='rotary':
            assert (len(src_state_dict.keys()) == 13)  # we are only accounting for 12 keys here
        # ln_1
        tgt_state_dict[f"transformer.h.{layer_idx - 1}.ln_1.weight"] = src_state_dict["input_layernorm.weight"]
        tgt_state_dict[f"transformer.h.{layer_idx - 1}.ln_1.bias"] = src_state_dict["input_layernorm.bias"]

        # attn.bias, attn.masked_bias: ignored

        # qkv_proj
        tgt_state_dict[f"transformer.h.{layer_idx - 1}.attn.qkv_proj.weight"] = src_state_dict[
            "attention.query_key_value.weight"]
        tgt_state_dict[f"transformer.h.{layer_idx - 1}.attn.qkv_proj.bias"] = src_state_dict[
            "attention.query_key_value.bias"]

        # out_proj
        tgt_state_dict[f"transformer.h.{layer_idx - 1}.attn.out_proj.weight"] = src_state_dict["attention.dense.weight"]
        tgt_state_dict[f"transformer.h.{layer_idx - 1}.attn.out_proj.bias"] = src_state_dict["attention.dense.bias"]

        # ln_2
        tgt_state_dict[f"transformer.h.{layer_idx - 1}.ln_2.weight"] = src_state_dict["post_attention_layernorm.weight"]
        tgt_state_dict[f"transformer.h.{layer_idx - 1}.ln_2.bias"] = src_state_dict["post_attention_layernorm.bias"]

        # mlp
        tgt_state_dict[f"transformer.h.{layer_idx - 1}.mlp.fc_in.weight"] = src_state_dict["mlp.dense_h_to_4h.weight"]
        tgt_state_dict[f"transformer.h.{layer_idx - 1}.mlp.fc_in.bias"] = src_state_dict["mlp.dense_h_to_4h.bias"]
        tgt_state_dict[f"transformer.h.{layer_idx - 1}.mlp.fc_out.weight"] = src_state_dict["mlp.dense_4h_to_h.weight"]
        tgt_state_dict[f"transformer.h.{layer_idx - 1}.mlp.fc_out.bias"] = src_state_dict["mlp.dense_4h_to_h.bias"]

    # final norm
    src_state_dict = torch.load(os.path.join(checkpoint_dir, f"layer_{num_layers + 3:02}-model_00-model_states.pt"))
    assert (len(src_state_dict.keys()) == 2)
    tgt_state_dict["transformer.ln_f.weight"] = src_state_dict["norm.weight"]
    tgt_state_dict["transformer.ln_f.bias"] = src_state_dict["norm.bias"]

    # output layer
    src_state_dict = torch.load(os.path.join(checkpoint_dir, f"layer_{num_layers + 4:02}-model_00-model_states.pt"))
    assert (len(src_state_dict.keys()) == 1)
    # Truncate the embedding table to vocab_size rows.
    final_embeddings = src_state_dict["final_linear.weight"]
    final_embeddings = final_embeddings[: config.vocab_size, :]
    tgt_state_dict["lm_head.weight"] = final_embeddings

    return tgt_state_dict

debug=False

if __name__ == "__main__":

    # Create the argument parser.
    if debug:
        args=mock_get_args()
    else:
        args=get_args()
    #
    chpnt_pth=Path(args.checkpoint_dir)
    config_dir = os.path.join(chpnt_pth, "configs")
    config_extract = convert_gpt_neox_config_to_hf(config_dir)
    config_extract.save_pretrained(args.hf_save_dir)
    state_dict = get_state_dict_from_checkpoint_dir(args.checkpoint_dir, config_extract)
    # load a lm_head model for saving
    if config_extract.pos_emb=='learned':
        model= GPTNeoXPosLearnedForCausalLM(config_extract)
    else:
        model = GPTNeoXForCausalLM(config_extract)
    missing_keys, unexpected_keys = model.load_state_dict(state_dict, strict=False)
    print(f"missing keys: {missing_keys}")
    print(f"unexpected keys: {unexpected_keys}")

    model.save_pretrained(args.hf_save_dir)