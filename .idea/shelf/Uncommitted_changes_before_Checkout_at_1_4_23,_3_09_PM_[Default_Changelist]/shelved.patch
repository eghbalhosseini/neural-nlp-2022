Index: neural_nlp/models/implementations.py
IDEA additional info:
Subsystem: com.intellij.openapi.diff.impl.patch.BaseRevisionTextPatchEP
<+>import copy\nfrom collections import OrderedDict, defaultdict\nfrom enum import Enum\nfrom importlib import import_module\n\nimport itertools\nimport logging\nimport numpy as np\nimport os\nimport pandas as pd\nimport pickle\nimport tempfile\nfrom brainio.fetch import fullname\nfrom numpy.random.mtrand import RandomState\nfrom pathlib import Path\nfrom tqdm import tqdm\n\nfrom brainscore.utils import LazyLoad\nfrom neural_nlp.models.wrapper.core import ActivationsExtractorHelper\nfrom neural_nlp.models.wrapper.pytorch import PytorchWrapper\nfrom neural_nlp.models.gpt_neox_model import GPTNeoXModel,GPTNeoXPosLearnedModel,GPTNeoXPosLearnedConfig,GPTNeoXConfig, initialize_gpt_neox_weights\nfrom neural_nlp.models import initialize_gpt2_weights\nfrom transformers import AutoConfig, AutoModel, AutoModelWithLMHead,AutoTokenizer\nAutoConfig.register('gpt-neox',GPTNeoXConfig)\nAutoConfig.register('gpt-neox-pos-learned',GPTNeoXPosLearnedConfig)\nAutoModel.register(GPTNeoXConfig, GPTNeoXModel)\nAutoModel.register(GPTNeoXPosLearnedConfig, GPTNeoXPosLearnedModel)\n\n\n_ressources_dir = (Path(__file__).parent / '..' / '..' / 'ressources' / 'models').resolve()\n\n\nclass BrainModel:\n    Modes = Enum('Mode', 'recording')\n\n    def __call__(self, sentences):\n        \"\"\"\n        Record representations in response to sentences. Ideally this would be localized to a\n        :param sentences:\n        :return:\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass TaskModel:\n    Modes = Enum('Mode', 'tokens_to_features sentence_features')\n\n    def __init__(self):\n        super(TaskModel, self).__init__()\n        self._mode = BrainModel.Modes.recording  # run as BrainModel by default\n\n    @property\n    def mode(self):\n        return self._mode\n\n    @mode.setter\n    def mode(self, value):\n        assert value in TaskModel.Modes or value in BrainModel.Modes\n        self._mode = value\n\n    def tokenize(self, text):\n        raise NotImplementedError()\n\n    def tokens_to_inputs(self, tokens):\n        return tokens\n\n    @property\n    def features_size(self):\n        raise NotImplementedError()\n\n    @property\n    def vocab_size(self):\n        raise NotImplementedError()\n\n    def glue_dataset(self, task, examples, label_list, output_mode, max_seq_length):\n        \"\"\"\n        :return: a torch TensorDataset where the last item is the labels\n        \"\"\"\n        raise NotImplementedError()\n\n\nclass SentenceLength(BrainModel, TaskModel):\n    \"\"\"\n    control model\n    \"\"\"\n    available_layers = ['sentence-length']\n    default_layers = available_layers\n\n    identifier = 'sentence-length'\n\n    def __init__(self):\n        super(SentenceLength, self).__init__()\n        self._extractor = ActivationsExtractorHelper(identifier=self.identifier,\n                                                     get_activations=self._get_activations, reset=lambda: None)\n\n    def __call__(self, *args, average_sentence=True, **kwargs):\n        if not average_sentence:\n            raise ValueError(\"This model only works on a sentence-level\")\n        return self._extractor(*args, **kwargs)\n\n    def _get_activations(self, sentences, layers):\n        np.testing.assert_array_equal(layers, self.available_layers)\n        sentence_lengths = [len(sentence.split(' ')) for sentence in sentences]\n        return {self.available_layers[0]: np.array(sentence_lengths)}\n\n\nclass WordPosition(BrainModel):\n    \"\"\"\n    control model\n    \"\"\"\n    available_layers = ['word-position']\n    default_layers = available_layers\n\n    identifier = 'word-position'\n\n    def __init__(self):\n        super(WordPosition, self).__init__()\n        self._extractor = ActivationsExtractorHelper(identifier=self.identifier,\n                                                     get_activations=self._get_activations, reset=lambda: None)\n\n    def __call__(self, *args, average_sentence=True, **kwargs):\n        if average_sentence:\n            raise ValueError(\"This model only works on a word-level\")\n        return self._extractor(*args, **kwargs)\n\n    def _get_activations(self, sentences, layers):\n        np.testing.assert_array_equal(layers, self.available_layers)\n        word_positions = [np.array([[[i] for i, word in enumerate(sentence.split(' '))]]) for sentence in sentences]\n        return {self.available_layers[0]: word_positions}\n\n\nclass RandomEmbedding(BrainModel):\n    \"\"\"\n    control model\n    \"\"\"\n    identifier = 'random-embedding'\n    available_layers = [identifier]\n    default_layers = available_layers\n\n    def __init__(self, num_embeddings=1600):\n        self._random_state = RandomState(0)\n        self._embeddings = defaultdict(lambda: self._random_state.rand(num_embeddings))\n        self._extractor = ActivationsExtractorHelper(identifier=self.identifier,\n                                                     get_activations=self._get_activations, reset=lambda: None)\n\n    def __call__(self, *args, average_sentence=True, **kwargs):\n        return _call_conditional_average(*args, extractor=self._extractor,\n                                         average_sentence=average_sentence, sentence_averaging=word_mean, **kwargs)\n\n    def _get_activations(self, sentences, layers):\n        np.testing.assert_array_equal(layers, self.available_layers)\n        word_embeddings = [np.array([[self._embeddings[word] for word in sentence.split()]]) for sentence in sentences]\n        return {self.available_layers[0]: word_embeddings}\n\n\nclass ETM(BrainModel, TaskModel):\n    \"\"\"\n    Dieng et al., 2019\n    https://arxiv.org/abs/1907.04907\n    \"\"\"\n\n    identifier = 'ETM'\n\n    available_layers = ['projection']\n    default_layers = available_layers\n\n    def __init__(self, weights_file='rho_100_20ng_min_df_2.npy', vocab_file='vocab_100_20ng_min_df_2.pkl',\n                 emb_size=300, random_embeddings=False, random_std=1):\n        super().__init__()\n        self._logger = logging.getLogger(fullname(self))\n\n        weights_file = os.path.join(_ressources_dir, 'topicETM', weights_file)\n        vocab_file = os.path.join(_ressources_dir, 'topicETM', vocab_file)\n        self.weights = np.load(weights_file)\n        self.emb_size = emb_size\n        with open(vocab_file, 'rb') as f:\n            self.vocab = pickle.load(f)\n        self.vocab_index = {word: index for index, word in enumerate(self.vocab)}\n        self.index_vocab = {index: word for index, word in enumerate(self.vocab)}\n\n        if random_embeddings:\n            self._logger.debug(f\"Replacing embeddings with random N(0, {random_std})\")\n            random_embedding = RandomState(0).randn(len(self.vocab), self.emb_size) * random_std\n            self.wordEmb_TopicSpace = {word: random_embedding[i] for i, word in enumerate(sorted(self.vocab))}\n        else:\n            wordEmb_TopicSpace = {}\n            for elm in tqdm(self.vocab, desc='vocab'):\n                i = self.vocab.index(elm)  # get index of word\n                wordEmb_TopicSpace[elm] = self.weights[i]\n            self.wordEmb_TopicSpace = wordEmb_TopicSpace\n\n        self._extractor = ActivationsExtractorHelper(\n            identifier=self.identifier + ('-untrained' if random_embeddings else ''),\n            get_activations=self._get_activations, reset=lambda: None)\n        self._extractor.insert_attrs(self)\n        self._logger = logging.getLogger(self.__class__.__name__)\n\n    def __call__(self, *args, average_sentence=True, **kwargs):\n        if self.mode == BrainModel.Modes.recording:\n            return _call_conditional_average(*args, extractor=self._extractor,\n                                             average_sentence=average_sentence, sentence_averaging=word_mean, **kwargs)\n        elif self.mode == TaskModel.Modes.tokens_to_features:\n            return self._encode_sentence(*args, **kwargs)\n\n    def _encode_sentence(self, sentence):\n        if isinstance(sentence, str):\n            words = sentence.split()\n        else:\n            words = [self.index_vocab[index] for index in sentence]\n        feature_vectors = []\n        for word in words:\n            word = word.lower()\n            if word in self.vocab:\n                feature_vectors.append(self.wordEmb_TopicSpace[word])\n            else:\n                self._logger.warning(f\"Word {word} not present in model\")\n                feature_vectors.append(np.zeros((self.emb_size,)))\n        return feature_vectors\n\n    def _get_activations(self, sentences, layers):\n        np.testing.assert_array_equal(layers, ['projection'])\n        encoding = [np.array(self._encode_sentence(sentence)) for sentence in sentences]\n        encoding = [np.expand_dims(sentence_encodings, 0) for sentence_encodings in encoding]\n        return {'projection': encoding}\n\n    def tokenize(self, text, vocab_size=None):\n        vocab_size = vocab_size or self.vocab_size\n        words = text.split()\n        tokens = [self.vocab_index[word] for word in tqdm(words, desc='tokenize') if word in self.vocab\n                  and self.vocab_index[word] < vocab_size]  # only top-k vocab words\n        return np.array(tokens)\n\n    def _sent_mean(self, sentence_features):\n        sent_mean = np.mean(sentence_features, axis=0)  # average across words within a sentence\n        return sent_mean\n\n    def glue_dataset(self, examples, label_list, output_mode):\n        import torch\n        from torch.utils.data import TensorDataset\n        label_map = {label: i for i, label in enumerate(label_list)}\n        features = []\n\n        if examples[0].text_b is not None:\n            text_a = [example.text_a for example in examples]\n            text_b = [example.text_b for example in examples]\n            sents1 = [self._sent_mean(self._encode_sentence(sent)) for sent in tqdm(text_a)]\n            sents2 = [self._sent_mean(self._encode_sentence(sent)) for sent in tqdm(text_b)]\n            for sent1, sent2 in zip(sents1, sents2):\n                sent1 = torch.tensor(sent1, dtype=torch.float64)\n                sent2 = torch.tensor(sent2, dtype=torch.float64)\n                if np.isnan(sent1).all():\n                    sent1 = torch.ones(self.emb_size, dtype=sent1.dtype)\n                if np.isnan(sent2).all():\n                    sent2 = torch.ones(self.emb_size, dtype=sent2.dtype)\n                f = torch.cat([sent1, sent2, torch.abs(sent1 - sent2), sent1 * sent2], -1)\n                features.append(PytorchWrapper._tensor_to_numpy(f))\n            all_features = torch.tensor(features).float()\n        else:\n            text_a = [example.text_a for example in examples]\n            sents = [self._sent_mean(self._encode_sentence(sent)) for sent in tqdm(text_a)]\n            for sent in sents:\n                sent = torch.tensor(sent)\n                features.append(PytorchWrapper._tensor_to_numpy(sent))\n            all_features = torch.tensor(features).float()\n\n        if output_mode == \"classification\":\n            all_labels = torch.tensor([label_map[example.label] for example in examples], dtype=torch.long)\n        elif output_mode == \"regression\":\n            all_labels = torch.tensor([float(example.label) for example in examples], dtype=torch.float)\n\n        dataset = TensorDataset(all_features, all_labels)\n        return dataset\n\n    @property\n    def features_size(self):\n        return self.emb_size\n\n    @property\n    def vocab_size(self):\n        return len(self.vocab)\n\n\nclass SkipThoughts(BrainModel, TaskModel):\n    \"\"\"\n    Kiros et al., 2015\n    http://papers.nips.cc/paper/5950-skip-thought-vectors\n    \"\"\"\n\n    identifier = 'skip-thoughts'\n\n    def __init__(self, weights=os.path.join(_ressources_dir, 'skip-thoughts'), load_weights=True, glue_batch_size=64):\n        super().__init__()\n        self._logger = logging.getLogger(fullname(self))\n        self._glue_batch_size = glue_batch_size\n        import skipthoughts\n        weights = weights + '/'\n        model = LazyLoad(lambda: skipthoughts.load_model(path_to_models=weights, path_to_tables=weights))\n        self._encoder = LazyLoad(lambda: skipthoughts.Encoder(model))\n        self._extractor = ActivationsExtractorHelper(\n            identifier=self.identifier + (\"-untrained\" if not load_weights else ''),\n            get_activations=self._get_activations, reset=lambda: None)  # resets states on its own\n        self._extractor.insert_attrs(self)\n        # setup prioritized vocabulary entries to map to indices and back.\n        # unfortunately it does not seem straight-forward to retrieve preferred words/tokens for the model, such as\n        # their frequency in the training dataset. The pretrained version we're using was trained on the BookCorpus\n        # dataset which is no longer online and any word frequencies did not seem to be saved in the files available\n        # here. The `encoder._model['utable']` is an OrderedDict, but the ordering does not intuitively seem to\n        # correspond to frequencies. Since there seems to be no other way, we will treat the utable as a\n        # frequency-ordered dict regardless and hope for the best.\n        self.vocab_index = {word: index for index, word in enumerate(self._encoder._model['utable'])}\n        self.index_vocab = {index: word for index, word in enumerate(self._encoder._model['utable'])}\n\n    @property\n    def vocab_size(self):\n        return len(self._encoder._model['utable'])\n\n    @property\n    def features_size(self):\n        return 4800\n\n    def tokenize(self, text, vocab_size=None):\n        if (bool(vocab_size)) and vocab_size < self.vocab_size:  # smaller vocab size requested, drop tokens\n            self._logger.debug(f\"Shortening {self.vocab_size} to {vocab_size}\")\n            _vocab_size = vocab_size\n        else:\n            _vocab_size = self.vocab_size\n        words = text.split()\n        tokens = [self.vocab_index[word] for word in tqdm(words, desc='tokenize') if word in self.vocab_index\n                  and self.vocab_index[word] < _vocab_size]  # only top-k vocab words\n        return np.array(tokens)\n\n    def __call__(self, *args, average_sentence=True, **kwargs):\n        if self.mode == BrainModel.Modes.recording:\n            return _call_conditional_average(*args, extractor=self._extractor,\n                                             average_sentence=average_sentence, sentence_averaging=word_last, **kwargs)\n        elif self.mode == TaskModel.Modes.tokens_to_features:\n            return self._encode_sentence(*args, **kwargs)\n\n    def _get_activations(self, sentences, layers):\n        np.testing.assert_array_equal(layers, self.available_layers)\n        encoding = [self._encode_sentence(sentence) for sentence in sentences]\n        return {'encoder': encoding}\n\n    def _encode_sentence(self, sentence):\n        if isinstance(sentence, str):\n            words = sentence.split(' ')\n        else:\n            words = [self.index_vocab[index] for index in sentence]\n        sentence_words = []\n        sentence_encoding = []\n        for word in words:\n            sentence_words.append(word)\n            word_embeddings = self._encoder.encode([' '.join(sentence_words)])\n            sentence_encoding.append(word_embeddings)\n        sentence_encoding = np.array(sentence_encoding).transpose([1, 0, 2])\n        return sentence_encoding\n\n    def glue_dataset(self, examples, label_list, output_mode):\n        import torch\n        from torch.utils.data import TensorDataset\n        label_map = {label: i for i, label in enumerate(label_list)}\n        features = []\n\n        if examples[0].text_b is not None:\n            for example_batch in tqdm(range(0, len(examples), self._glue_batch_size)):\n                batch_examples = examples[example_batch:example_batch + self._glue_batch_size]\n                text_a = [batch_example.text_a for batch_example in batch_examples]\n                text_b = [batch_example.text_b for batch_example in batch_examples]\n                try:\n                    sents1 = self._encoder.encode(text_a)  # sentences x features encoding\n                    sents2 = self._encoder.encode(text_b)\n                except ValueError:  # QQP seems to have an empty line which we'll just ignore\n                    self._logger.warning(f\"Ignoring ValueError at batch {example_batch}\", exc_info=True)\n                for sent1, sent2 in zip(sents1, sents2):\n                    sent1 = torch.tensor(sent1)\n                    sent2 = torch.tensor(sent2)\n                    if np.isnan(sent1).all():\n                        sent1 = torch.ones(sent2.shape, dtype=sent1.dtype)\n                    if np.isnan(sent2).all():\n                        sent2 = torch.ones(sent1.shape, dtype=sent2.dtype)\n                    f = torch.cat([sent1, sent2, torch.abs(sent1 - sent2), sent1 * sent2], -1)\n                    features.append(PytorchWrapper._tensor_to_numpy(f))\n            all_features = torch.tensor(features)\n        else:\n            for example_batch in tqdm(range(0, len(examples), self._glue_batch_size)):\n                batch_examples = examples[example_batch:example_batch + self._glue_batch_size]\n                text_a = [batch_example.text_a for batch_example in batch_examples]\n                sents = self._encoder.encode(text_a)\n                for sent in sents:\n                    sent = torch.tensor(sent)\n                    features.append(PytorchWrapper._tensor_to_numpy(sent))\n            all_features = torch.tensor(features)\n\n        if output_mode == \"classification\":\n            all_labels = torch.tensor([label_map[example.label] for example in examples], dtype=torch.long)\n        elif output_mode == \"regression\":\n            all_labels = torch.tensor([float(example.label) for example in examples], dtype=torch.float)\n            # https://github.com/huggingface/transformers/blob/master/src/transformers/data/processors/glue.py#L138\n\n        dataset = TensorDataset(all_features, all_labels)\n        return dataset\n\n    available_layers = ['encoder']\n    default_layers = available_layers\n\n\nclass LM1B(BrainModel, TaskModel):\n    \"\"\"\n    Jozefowicz et al., 2016\n    https://arxiv.org/pdf/1602.02410.pdf\n    \"\"\"\n\n    identifier = 'lm_1b'\n\n    def __init__(self, weights=os.path.join(_ressources_dir, 'lm_1b'), reset_weights=False):\n        super().__init__()\n        self._logger = logging.getLogger(self.__class__.__name__)\n        from lm_1b.lm_1b_eval import Encoder\n        self._encoder = Encoder(vocab_file=os.path.join(weights, 'vocab-2016-09-10.txt'),\n                                pbtxt=os.path.join(weights, 'graph-2016-09-10.pbtxt'),\n                                ckpt=os.path.join(weights, 'ckpt-*'),\n                                reset_weights=reset_weights)\n        self._extractor = ActivationsExtractorHelper(\n            identifier=self.identifier + ('-untrained' if reset_weights else ''),\n            get_activations=self._get_activations, reset=self._initialize)\n        self._extractor.insert_attrs(self)\n        self._vocab_index = self._encoder.vocab._word_to_id\n        self._index_vocab = {index: word for word, index in self._vocab_index.items()}\n\n    @property\n    def vocab_size(self):\n        return len(self._vocab_index)\n\n    @property\n    def features_size(self):\n        return 1024\n\n    def tokenize(self, text, vocab_size=None):\n        if (bool(vocab_size)) and vocab_size < self.vocab_size:  # smaller vocab size requested, drop tokens\n            self._logger.debug(f\"Shortening {self.vocab_size} to {vocab_size}\")\n            _vocab_size = vocab_size\n        else:\n            _vocab_size = self.vocab_size\n        words = text.split()\n        tokens = [self._vocab_index[word] for word in tqdm(words, desc='tokenize')\n                  if word in self._vocab_index and self._vocab_index[word] < _vocab_size]  # only top-k vocab words\n        return np.array(tokens)\n\n    def __call__(self, *args, average_sentence=True, **kwargs):\n        if self.mode == BrainModel.Modes.recording:\n            return _call_conditional_average(*args, extractor=self._extractor,\n                                             average_sentence=average_sentence, sentence_averaging=word_last, **kwargs)\n        elif self.mode == TaskModel.Modes.tokens_to_features:\n            self._initialize()  # reset\n            readout_layer = self.default_layers[-1]\n            return self._encode_sentence(*args, layers=[readout_layer], **kwargs)[readout_layer]\n\n    def _get_activations(self, sentences, layers):\n        layer_activations = defaultdict(list)\n        for sentence in sentences:\n            embeddings = self._encode_sentence(sentence, layers)\n            for layer, layer_embeddings in embeddings.items():\n                layer_activations[layer].append(np.array([layer_embeddings]))\n        return layer_activations\n\n    def _encode_sentence(self, sentence, layers):\n        from lm_1b import lm_1b_eval\n        from six.moves import xrange\n        if not isinstance(sentence, str):\n            sentence = ' '.join([self._index_vocab[index] for index in sentence])\n        self._initialize()\n        # the following is copied from lm_1b.lm_1b_eval.Encoder.__call__.\n        # only the `sess.run` call needs to be changed but there's no way to access it outside the code\n        if sentence.find('<S>') != 0:\n            sentence = '<S> ' + sentence\n        word_ids = [self._encoder.vocab.word_to_id(w) for w in sentence.split()]\n        char_ids = [self._encoder.vocab.word_to_char_ids(w) for w in sentence.split()]\n        # some unknown characters end up as '�' (ord 65533). Replace those with empty (4)\n        char_ids = [np.array([c if c < 256 else 4 for c in chars]) for chars in char_ids]\n        inputs = np.zeros([lm_1b_eval.BATCH_SIZE, lm_1b_eval.NUM_TIMESTEPS], np.int32)\n        char_ids_inputs = np.zeros(\n            [lm_1b_eval.BATCH_SIZE, lm_1b_eval.NUM_TIMESTEPS, self._encoder.vocab.max_word_length], np.int32)\n        embeddings = []\n        targets = np.zeros([lm_1b_eval.BATCH_SIZE, lm_1b_eval.NUM_TIMESTEPS], np.int32)\n        weights = np.ones([lm_1b_eval.BATCH_SIZE, lm_1b_eval.NUM_TIMESTEPS], np.float32)\n        for i in xrange(len(word_ids)):\n            inputs[0, 0] = word_ids[i]\n            char_ids_inputs[0, 0, :] = char_ids[i]\n            # calling this repeatedly with the same input leads to different embeddings,\n            # so we infer this preserves hidden state\n            lstm_emb = self._encoder.sess.run([self._encoder.t[name] for name in layers],\n                                              feed_dict={self._encoder.t['char_inputs_in']: char_ids_inputs,\n                                                         self._encoder.t['inputs_in']: inputs,\n                                                         self._encoder.t['targets_in']: targets,\n                                                         self._encoder.t['target_weights_in']: weights})\n            if i > 0:  # 0 is <S>\n                embeddings.append(lstm_emb)\n            # `embeddings` shape is now: words x layers x *layer_shapes\n        layer_activations = {}\n        for i, layer in enumerate(layers):\n            # embeddings is `words x layers x (1 x 1024)`\n            layer_activations[layer] = [embedding[i] for embedding in embeddings]\n            # `words x 1 x 1024` --> `words x 1024`\n            layer_activations[layer] = np.array(layer_activations[layer]).transpose(1, 0, 2).squeeze(axis=0)\n        return layer_activations\n\n    def _initialize(self):\n        self._encoder.sess.run(self._encoder.t['states_init'])\n\n    def available_layers(self, filter_inputs=True):\n        return [tensor_name for tensor_name in self._encoder.t if not filter_inputs or not tensor_name.endswith('_in')]\n\n    def glue_dataset(self, examples, label_list, output_mode):\n        import torch\n        from torch.utils.data import TensorDataset\n        label_map = {label: i for i, label in enumerate(label_list)}\n        features = []\n\n        if examples[0].text_b is not None:\n            text_a = [example.text_a for example in examples]\n            text_b = [example.text_b for example in examples]\n            sents1 = self._encoder(text_a, progress_bar=True)[0]\n            sents2 = self._encoder(text_b, progress_bar=True)[0]\n            for sent1, sent2 in zip(sents1, sents2):\n                sent1 = torch.tensor(sent1[-1].squeeze())\n                sent2 = torch.tensor(sent2[-1].squeeze())\n                if np.isnan(sent1).all():\n                    sent1 = torch.ones(sent2.shape, dtype=sent1.dtype)\n                if np.isnan(sent2).all():\n                    sent2 = torch.ones(sent1.shape, dtype=sent2.dtype)\n                f = torch.cat([sent1, sent2, torch.abs(sent1 - sent2), sent1 * sent2], -1)\n                features.append(PytorchWrapper._tensor_to_numpy(f))\n            all_features = torch.tensor(features)\n\n        else:\n            text_a = [example.text_a for example in examples]\n            # encoder returns a tuple (sentences_embeddings, sentences_word_ids).\n            # the sentences_embeddings encode every single word with a 1x1024 embedding,\n            # so we use the last word's embedding and squeeze the singular dimension.\n            sents = self._encoder(text_a, progress_bar=True)[0]\n            for sent in sents:\n                sent = torch.tensor(sent[-1].squeeze())\n                features.append(PytorchWrapper._tensor_to_numpy(sent))\n            all_features = torch.tensor(features)\n\n        if output_mode == \"classification\":\n            all_labels = torch.tensor([label_map[example.label] for example in examples], dtype=torch.long)\n        elif output_mode == \"regression\":\n            all_labels = torch.tensor([label_map[example.label] for example in examples], dtype=torch.float)\n\n        dataset = TensorDataset(all_features, all_labels)\n        return dataset\n\n    default_layers = ['lstm/lstm_0/control_dependency', 'lstm/lstm_1/control_dependency']\n\n\ndef word_last(layer_activations):\n    for layer, activations in layer_activations.items():\n        assert all(a.shape[0] == 1 for a in activations)\n        activations = [a[0, -1, :] for a in activations]\n        layer_activations[layer] = np.array(activations)\n    return layer_activations\n\n\ndef word_mean(layer_activations):\n    for layer, activations in layer_activations.items():\n        activations = [np.mean(a, axis=1) for a in activations]  # average across words within a sentence\n        layer_activations[layer] = np.concatenate(activations)\n    return layer_activations\n\n\nclass Transformer(PytorchWrapper, BrainModel, TaskModel):\n    \"\"\"\n    Vaswani & Shazeer & Parmar & Uszkoreit & Jones & Gomez & Kaiser & Polosukhin, 2017\n    https://arxiv.org/pdf/1706.03762.pdf\n    \"\"\"\n\n    identifier = 'transformer'\n\n    def __init__(self, untrained=False):\n        weights = os.path.join(_ressources_dir, 'transformer/averaged-10-epoch.pt')\n        from onmt.opts import add_md_help_argument, translate_opts\n        from onmt.translate.translator import build_translator\n        import argparse\n        parser = argparse.ArgumentParser(description='transformer-parser-base')\n        add_md_help_argument(parser)\n        translate_opts(parser, weights)\n        opt = parser.parse_args(['-batch_size', '1'])\n        translator = build_translator(opt, report_score=True, untrained=untrained)\n\n        self._model_container = self.TransformerContainer(translator, opt)\n        self.vocab_index = {word: index for index, word in\n                            enumerate(self._model_container.translator.fields[\"src\"].vocab.freqs)}\n        index_vocab = {index: word for word, index in self.vocab_index.items()}\n        self._model_container.index_vocab = index_vocab\n        super(Transformer, self).__init__(\n            identifier=self.identifier + ('-untrained' if untrained else ''),\n            model=self._model_container, reset=lambda: None)  # transformer is feed-forward\n\n    def __call__(self, *args, average_sentence=True, **kwargs):\n        if self.mode == BrainModel.Modes.recording:\n            return _call_conditional_average(*args, extractor=self._extractor,\n                                             average_sentence=average_sentence, sentence_averaging=word_last, **kwargs)\n        elif self.mode == TaskModel.Modes.tokens_to_features:\n            encodings = self._model_container(*args, **kwargs)\n            # the onmt implementation concats things together, undo this\n            return encodings[0].reshape(-1, self.features_size)\n\n    class TransformerContainer:\n        def __init__(self, translator, opt):\n            self.translator = translator\n            self.opt = opt\n            self.index_vocab = None\n\n        def __getattr__(self, name):\n            return getattr(self.translator.model, name)\n\n        def __call__(self, sentences):\n            with tempfile.NamedTemporaryFile(mode='w+') as file:\n                # separating sentences with newline, combined with a batch size of 1\n                # will lead to one set of activations per sentence (albeit multiple words).\n                if isinstance(sentences, np.ndarray) and not isinstance(sentences[0], str):\n                    sentences = [\" \".join([self.index_vocab[index] for index in sentences])]\n                file.write('\\n'.join(sentences) + '\\n')\n                file.flush()\n                encodings = self.translator.get_encodings(src_path=file.name, tgt_path=self.opt.tgt,\n                                                          src_dir=self.opt.src_dir, batch_size=self.opt.batch_size,\n                                                          attn_debug=self.opt.attn_debug)\n                return encodings\n\n    def register_hook(self, layer, layer_name, target_dict):\n        def hook_function(_layer, _input, output, name=layer_name):\n            numpy_output = PytorchWrapper._tensor_to_numpy(output)\n            target_dict[name].append(numpy_output)\n\n        hook = layer.register_forward_hook(hook_function)\n        return hook\n\n    \"\"\"\n    For each of the 6 encoder blocks, we're using two layers,\n    one following the Multi-Head Attention and one following the Feed Forward block (cf. Figure 1).\n\n    The encoder is implemented as follows:\n    ```\n    input_norm = self.layer_norm(inputs)\n    context, _ = self.self_attn(input_norm, input_norm, input_norm, mask=mask)\n    out = self.dropout(context) + inputs\n    return self.feed_forward(out)\n    ```\n    `feed_forward` is implemented as follows:\n    ```\n    inter = self.dropout_1(self.relu(self.w_1(self.layer_norm(x))))\n    output = self.dropout_2(self.w_2(inter))\n    return output + x\n    ```\n    We thus use `feed_forward.layer_norm` as the layer immediately following the Multi-Head Attention\n    and `feed_forward.dropout_2` as the last layer of the Feed Forward block.\n    Note however that the attended input has not yet been added back to the feed forward output with\n    `feed_forward.dropout_2`; with this framework we cannot capture that operation (we'd have to change the code).\n    \"\"\"\n    default_layers = [f'encoder.transformer.{i}.{layer}'\n                      for i in range(6) for layer in ['feed_forward.layer_norm', 'feed_forward.dropout_2']]\n\n    def tokenize(self, text, vocab_size=None):\n        assert not vocab_size or vocab_size == self.vocab_size\n        words = text.split()\n        tokens = [self.vocab_index[word] for word in tqdm(words, desc='tokenize') if word in self.vocab_index]\n        return np.array(tokens)\n\n    @property\n    def features_size(self):\n        return 512  # encoding output of onmt transformer\n\n    @property\n    def vocab_size(self):\n        return len(self._model_container.translator.fields[\"src\"].vocab)\n\n\nclass _PytorchTransformerWrapper(BrainModel, TaskModel):\n    def __init__(self, identifier, tokenizer, model, layers, sentence_average, tokenizer_special_tokens=()):\n        super(_PytorchTransformerWrapper, self).__init__()\n        self._logger = logging.getLogger(fullname(self))\n        self.default_layers = self.available_layers = layers\n        self._tokenizer = tokenizer\n        self._model = model\n        self._layers = layers\n        self._model_container = self.ModelContainer(tokenizer, model, layers, tokenizer_special_tokens)\n        self._sentence_average = sentence_average\n        self._extractor = ActivationsExtractorHelper(identifier=identifier, get_activations=self._model_container,\n                                                     reset=lambda: None)\n        self._extractor.insert_attrs(self)\n\n    def __call__(self, *args, average_sentence=True, **kwargs):\n        self._model.eval()\n        if self.mode == BrainModel.Modes.recording:\n            return _call_conditional_average(*args, extractor=self._extractor, average_sentence=average_sentence,\n                                             sentence_averaging=self._sentence_average, **kwargs)\n        elif self.mode == TaskModel.Modes.tokens_to_features:\n            return self._tokens_to_features(*args, **kwargs)\n        elif self.mode == TaskModel.Modes.sentence_features:\n            return self._sentence_features(*args, **kwargs)\n        else:\n            raise ValueError(f\"Unknown mode {self.mode}\")\n\n    def _tokens_to_features(self, token_ids, layer=None):\n        import torch\n        max_num_words = 512 - 2  # -2 for [cls], [sep]\n        if os.getenv('ALLATONCE', '0') == '1':\n            token_tensor = torch.tensor([token_ids])\n            token_tensor = token_tensor.to('cuda' if torch.cuda.is_available() else 'cpu')\n            features = self._model(token_tensor)[0][0]\n            features = PytorchWrapper._tensor_to_numpy(features)\n            return features\n        features = []\n        for token_index in range(len(token_ids)):\n            context_start = max(0, token_index - max_num_words + 1)\n            context_ids = token_ids[context_start:token_index + 1]\n            tokens_tensor = torch.tensor([context_ids])\n            tokens_tensor = tokens_tensor.to('cuda' if torch.cuda.is_available() else 'cpu')\n            #overall_features, _, context_encoding = self._model(tokens_tensor)\n            #model_out=self._model(tokens_tensor)\n            #overall_features=model_out[0]\n            #context_encoding=model_out[2]\n            #(overall_features,_, context_encoding) = self._model(tokens_tensor).to_tuple()\n            (overall_features, context_encoding) = self._model(tokens_tensor).to_tuple()\n            if layer is None:\n                context_features = overall_features[:, -1, :]\n            else:\n                layer_pos = self._layers.index(layer)\n                context_features = context_encoding[layer_pos][:, -1, :]\n            features.append(PytorchWrapper._tensor_to_numpy(context_features))\n        return np.concatenate(features)\n\n    def _sentence_features(self, batch):\n        import torch\n        inputs = {\"input_ids\": batch[0], \"attention_mask\": batch[1]}\n        if not any(self.identifier.startswith(prefix) for prefix in ['distilbert', 't5']):\n            inputs[\"token_type_ids\"] = (\n                batch[2] if not any(self.identifier.startswith(prefix) for prefix in\n                                    [\"bert\", \"xlnet\", \"albert\", \"roberta\", \"distilroberta\", \"xlm-roberta\"])\n                else None)  # XLM, DistilBERT, RoBERTa, and XLM-RoBERTa don't use segment_ids\n        if self.identifier.startswith('t5'):\n            # we already have a T5Wrapper in place that will convert input_ids to {encoder,decoder}_input_ids\n            inputs['encoder_attention_mask'] = inputs['attention_mask']\n            del inputs['attention_mask']\n        if self.identifier.startswith('transfo-xl-wt103'):\n            # transfo-xl does not accept arguments for attention_mask or token_type_ids\n            del inputs['attention_mask']\n            del inputs['token_type_ids']\n        with torch.no_grad():\n            features_outputs = self._model(**inputs)\n        # https://github.com/huggingface/transformers/blob/520e7f211926e07b2059bc8e21b668db4372e4db/src/transformers/modeling_bert.py#L811-L812\n        sequence_output = features_outputs[0]\n        if any(self.identifier.startswith(first_token_model) for first_token_model in\n               ['bert', 'roberta', 'xlm', 'albert', 'distilbert', 'distilroberta']):\n            # https://github.com/huggingface/transformers/blob/520e7f211926e07b2059bc8e21b668db4372e4db/src/transformers/modeling_bert.py#L454\n            return sequence_output[:, 0]  # sentence features from first token (usually CLS)\n        elif any(self.identifier.startswith(last_token_model) for last_token_model in\n                 ['distilgpt2', 'openaigpt', 'gpt', 'xlnet', 'ctrl', 'transfo-xl-wt103', 't5']):\n            # use the last \"real\" token, ignoring padding by checking attention\n            last_attended_token = []\n            for batch in range(sequence_output.shape[0]):  # padding can be different per batch element\n                if 'attention_mask' in inputs:  # only use features of last attended input\n                    attention = inputs['attention_mask'][batch]\n                    last_attention = torch.where(attention)[0].max()  # last index where attention is non-zero\n                else:  # no attention mask given, rely on last non-padding token instead\n                    pad_id = self._tokenizer.convert_tokens_to_ids(\n                        [self._tokenizer.pad_token if self._tokenizer.pad_token else ''])[0]\n                    first_pad_index = torch.where(inputs['input_ids'][batch] == pad_id)[0]\n                    if len(first_pad_index) > 0:  # padding found\n                        last_attention = first_pad_index[0] - 1  # use last token before padding started\n                    else: # no padding in input\n                        last_attention = sequence_output.shape[1] - 1  # use last token (no padding)\n                batch_token = sequence_output[batch, last_attention, :]\n                last_attended_token.append(batch_token)\n            last_attended_token = torch.stack(last_attended_token)\n            return last_attended_token\n        else:\n            raise NotImplementedError(f\"undefined if {self.identifier} should use \"\n                                      \"first or last token for sentence features\")\n\n    @property\n    def identifier(self):\n        return self._extractor.identifier\n\n    def tokenize(self, text, vocab_size=None):\n        tokenized_text = self._tokenizer.convert_tokens_to_ids(self._tokenizer.tokenize(text))\n        tokenized_text = np.array(tokenized_text)  # ~10 sec with numpy, ~40 hours without\n        if (bool(vocab_size)) and vocab_size < self.vocab_size:  # smaller vocab size requested, drop tokens\n            self._logger.debug(f\"Shortening {self.vocab_size} to {vocab_size} (max in tokens: {max(tokenized_text)})\")\n            tokenized_text = np.array([token for token in tokenized_text if token < vocab_size])\n        return tokenized_text\n\n    def tokens_to_inputs(self, tokens):\n        return np.array(self._tokenizer.build_inputs_with_special_tokens(tokens.tolist()))\n\n    def glue_dataset(self, task, examples, label_list, output_mode, max_seq_length):\n        import torch\n        from torch.utils.data import TensorDataset\n        from transformers import glue_convert_examples_to_features as convert_examples_to_features\n        if task in [\"mnli\", \"mnli-mm\"] and \\\n                any(self.identifier.startswith(swap_model) for swap_model in [\"roberta\", \"xlm-roberta\"]):\n            # HACK(label indices are swapped in RoBERTa pretrained model)\n            label_list[1], label_list[2] = label_list[2], label_list[1]\n        features = convert_examples_to_features(\n            examples,\n            self._tokenizer,\n            label_list=label_list,\n            max_length=max_seq_length,\n            output_mode=output_mode,\n            pad_on_left=bool(self.identifier.startswith(\"xlnet\")),  # pad on the left for xlnet\n            pad_token=self._tokenizer.convert_tokens_to_ids([self._tokenizer.pad_token\n                                                             if self._tokenizer.pad_token else ''])[0],\n            pad_token_segment_id=4 if self.identifier.startswith(\"xlnet\") else 0,\n        )\n\n        # Convert to Tensors and build dataset\n        all_input_ids = torch.tensor([f.input_ids for f in features], dtype=torch.long)\n        all_attention_mask = torch.tensor([f.attention_mask for f in features], dtype=torch.long)\n        all_token_type_ids = torch.tensor([f.token_type_ids for f in features], dtype=torch.long)\n        if output_mode == \"classification\":\n            all_labels = torch.tensor([f.label for f in features], dtype=torch.long)\n        elif output_mode == \"regression\":\n            all_labels = torch.tensor([f.label for f in features], dtype=torch.float)\n\n        dataset = TensorDataset(all_input_ids, all_attention_mask, all_token_type_ids, all_labels)\n        return dataset\n\n    @property\n    def features_size(self):\n        return self._model.config.hidden_size\n\n    @property\n    def vocab_size(self):\n        return self._model.config.vocab_size\n\n    def get_embedding_weights(self):\n        modules = list(self._model.modules())\n        while len(modules) > 1:  # 0th module is self\n            modules = list(modules[1].modules())\n        embedding_layer = modules[0]\n        return embedding_layer.weight\n\n    class ModelContainer:\n        def __init__(self, tokenizer, model, layer_names, tokenizer_special_tokens=()):\n            import torch\n            self.tokenizer = tokenizer\n            self.model = model.to('cuda' if torch.cuda.is_available() else 'cpu')\n            self.layer_names = layer_names\n            self.tokenizer_special_tokens = tokenizer_special_tokens\n\n        def __call__(self, sentences, layers):\n            import torch\n            self.model.eval()\n            num_words = [len(sentence.split()) for sentence in sentences]\n            text = copy.deepcopy(sentences)\n            additional_tokens = []\n            # If the tokenizer has a `cls_token`, we insert a `cls_token` at the beginning of the text\n            # and a [SEP] token at the end of the text. For models without a `cls_token`, no tokens are inserted.\n            use_special_tokens = self.tokenizer.cls_token is not None\n            if use_special_tokens:\n                additional_tokens += [self.tokenizer.cls_token, self.tokenizer.sep_token]\n                if len(text) > 0:\n                    text[0] = self.tokenizer.cls_token + text[0]\n                    text[-1] = text[-1] + self.tokenizer.sep_token\n\n            # Tokenized input\n            tokenized_sentences = [self.tokenizer.tokenize(sentence) for sentence in text]\n            # chain\n            tokenized_sentences = list(itertools.chain.from_iterable(tokenized_sentences))\n            tokenized_sentences = np.array(tokenized_sentences)\n            # mapping from original text to later undo chain\n            sentence_indices = [0] + [sum(num_words[:i]) for i in range(1, len(num_words), 1)]\n\n            max_num_words = 512 if not use_special_tokens else 511\n            aligned_tokens = self.align_tokens(\n                tokenized_sentences=tokenized_sentences, sentences=sentences,\n                max_num_words=max_num_words, additional_tokens=additional_tokens, use_special_tokens=use_special_tokens)\n            encoded_layers = [[]] * len(self.layer_names)\n            for context_ids in aligned_tokens:\n                # Convert inputs to PyTorch tensors\n                tokens_tensor = torch.tensor([context_ids])\n                tokens_tensor = tokens_tensor.to('cuda' if torch.cuda.is_available() else 'cpu')\n\n                # Predict hidden states features for each layer\n                with torch.no_grad():\n                    # output is ['last_hidden_state', 'past_key_values', 'hidden_states'] so [-1:] is the hidden_states\n                    context_encoding, = self.model(tokens_tensor).to_tuple()[-1:]\n                # We have a hidden state for all the layers\n                #print(f\" context length :{len(context_encoding)} vs layer length {len(self.layer_names)}\\n\")\n                assert len(context_encoding) == len(self.layer_names)\n                # take only the encoding of the current word index\n                word_encoding = [encoding[:, -1:, :] for encoding in context_encoding]\n                word_encoding = [PytorchWrapper._tensor_to_numpy(encoding) for encoding in word_encoding]\n                encoded_layers = [previous_words + [word_layer_encoding] for previous_words, word_layer_encoding\n                                  in zip(encoded_layers, word_encoding)]\n            encoded_layers = [np.concatenate(layer_encoding, axis=1) for layer_encoding in encoded_layers]\n            assert all(layer_encoding.shape[1] == sum(num_words) for layer_encoding in encoded_layers)\n            # separate into sentences again\n            sentence_encodings = [[layer_encoding[:, start:end, :] for start, end in\n                                   zip(sentence_indices, sentence_indices[1:] + [sum(num_words)])]\n                                  for layer_encoding in encoded_layers]\n            sentence_encodings = OrderedDict(zip(self.layer_names, sentence_encodings))\n            sentence_encodings = OrderedDict([(layer, encoding) for layer, encoding in sentence_encodings.items()\n                                              if layer in layers])\n            return sentence_encodings\n\n        def align_tokens(self, tokenized_sentences, sentences, max_num_words, additional_tokens, use_special_tokens):\n            # sliding window approach (see https://github.com/google-research/bert/issues/66)\n            # however, since this is a brain model candidate, we don't let it see future words (just like the brain\n            # doesn't receive future word input). Instead, we maximize the past context of each word\n            sentence_index = 0\n            sentences_chain = ' '.join(sentences).split()\n            previous_indices = []\n\n            for token_index in tqdm(range(len(tokenized_sentences)), desc='token features'):\n                if tokenized_sentences[token_index] in additional_tokens:\n                    continue  # ignore altogether\n                # combine e.g. \"'hunts', '##man'\" or \"'jennie', '##s'\"\n                tokens = [\n                    # tokens are sometimes padded by prefixes, clear those here\n                    word.lstrip('##').lstrip('▁').rstrip('@@')\n                    for word in tokenized_sentences[previous_indices + [token_index]]]\n                token_word = ''.join(tokens).lower()\n                for special_token in self.tokenizer_special_tokens:\n                    token_word = token_word.replace(special_token, '')\n                if sentences_chain[sentence_index].lower() != token_word:\n                    previous_indices.append(token_index)\n                    continue\n                previous_indices = []\n                sentence_index += 1\n\n                context_start = max(0, token_index - max_num_words + 1)\n                context = tokenized_sentences[context_start:token_index + 1]\n                if use_special_tokens and context_start > 0:  # `cls_token` has been discarded\n                    # insert `cls_token` again following\n                    # https://huggingface.co/pytorch-transformers/model_doc/roberta.html#pytorch_transformers.RobertaModel\n                    context = np.insert(context, 0, tokenized_sentences[0])\n                context_ids = self.tokenizer.convert_tokens_to_ids(context)\n                yield context_ids\n\n\nclass KeyedVectorModel(BrainModel, TaskModel):\n    \"\"\"\n    Lookup-table-like models where each word has an embedding.\n    To retrieve the sentence activation, we take the mean of the word embeddings.\n    \"\"\"\n\n    available_layers = ['projection']\n    default_layers = available_layers\n\n    def __init__(self, identifier, weights_file, random_embeddings=False, random_std=1, binary=False):\n        super().__init__()\n        self._logger = logging.getLogger(self.__class__.__name__)\n        from gensim.models.keyedvectors import KeyedVectors\n        self._model = KeyedVectors.load_word2vec_format(weights_file, binary=binary)\n        self._vocab = self._model.vocab\n        self._index2word_set = set(self._model.index2word)\n        if random_embeddings:\n            self._logger.debug(f\"Replacing embeddings with random N(0, {random_std})\")\n            random_embedding = RandomState(0).randn(len(self._index2word_set), len(self._model['the'])) * random_std\n            self._model = {word: random_embedding[i] for i, word in enumerate(sorted(self._index2word_set))}\n        self._extractor = ActivationsExtractorHelper(identifier=identifier, get_activations=self._get_activations,\n                                                     reset=lambda: None)\n        self._extractor.insert_attrs(self)\n\n    def __call__(self, stimuli, *args, average_sentence=True, **kwargs):\n        if self.mode == BrainModel.Modes.recording:\n            return _call_conditional_average(stimuli, *args, extractor=self._extractor,\n                                             average_sentence=average_sentence, sentence_averaging=word_mean, **kwargs)\n        elif self.mode == TaskModel.Modes.tokens_to_features:\n            stimuli = \" \".join(self._model.index2word[index] for index in stimuli)\n            return self._encode_sentence(stimuli, *args, **kwargs)\n\n    def _get_activations(self, sentences, layers):\n        np.testing.assert_array_equal(layers, ['projection'])\n        encoding = [np.array(self._encode_sentence(sentence)) for sentence in sentences]\n        # expand \"batch\" dimension for compatibility with transformers (for sentence-word-aggregation)\n        encoding = [np.expand_dims(sentence_encodings, 0) for sentence_encodings in encoding]\n        return {'projection': encoding}\n\n    def _encode_sentence(self, sentence):\n        words = sentence.split()\n        feature_vectors = []\n        for word in words:\n            if word in self._index2word_set:\n                feature_vectors.append(self._model[word])\n            else:\n                self._logger.warning(f\"Word {word} not present in model\")\n                feature_vectors.append(np.zeros((300,)))\n        return feature_vectors\n\n    def tokenize(self, text, vocab_size=None):\n        vocab_size = vocab_size or self.vocab_size\n        tokens = [self._vocab[word].index for word in text.split() if word in self._vocab\n                  and self._vocab[word].index < vocab_size]  # only top-k vocab words\n        return np.array(tokens)\n\n    def _sent_mean(self, sentence_features):\n        sent_mean = np.mean(sentence_features, axis=0)  # average across words within a sentence\n        return sent_mean\n\n    def glue_dataset(self, examples, label_list, output_mode):\n        import torch\n        from torch.utils.data import TensorDataset\n        label_map = {label: i for i, label in enumerate(label_list)}\n        features = []\n\n        if examples[0].text_b is not None:\n            text_a = [example.text_a for example in examples]\n            text_b = [example.text_b for example in examples]\n            sents1 = [self._sent_mean(self._encode_sentence(sent)) for sent in tqdm(text_a)]\n            sents2 = [self._sent_mean(self._encode_sentence(sent)) for sent in tqdm(text_b)]\n            for sent1, sent2 in zip(sents1, sents2):\n                sent1 = torch.tensor(sent1, dtype=torch.float64)\n                sent2 = torch.tensor(sent2, dtype=torch.float64)\n                if np.isnan(sent1).all():\n                    sent1 = torch.ones(sent2.shape, dtype=sent1.dtype)\n                if np.isnan(sent2).all():\n                    sent2 = torch.ones(sent1.shape, dtype=sent2.dtype)\n                f = torch.cat([sent1, sent2, torch.abs(sent1 - sent2), sent1 * sent2], -1)\n                features.append(PytorchWrapper._tensor_to_numpy(f))\n            all_features = torch.tensor(features).float()\n        else:\n            text_a = [example.text_a for example in examples]\n            sents = [self._sent_mean(self._encode_sentence(sent)) for sent in tqdm(text_a)]\n            for sent in sents:\n                sent = torch.tensor(sent)\n                features.append(PytorchWrapper._tensor_to_numpy(sent))\n            all_features = torch.tensor(features).float()\n\n        if output_mode == \"classification\":\n            all_labels = torch.tensor([label_map[example.label] for example in examples], dtype=torch.long)\n        elif output_mode == \"regression\":\n            all_labels = torch.tensor([float(example.label) for example in examples], dtype=torch.float)\n\n        dataset = TensorDataset(all_features, all_labels)\n        return dataset\n\n    @property\n    def vocab_size(self):\n        return len(self._vocab)\n\n    @property\n    def features_size(self):\n        return 300\n\n\nclass Word2Vec(KeyedVectorModel):\n    \"\"\"\n    Mikolov et al., 2013\n    https://arxiv.org/pdf/1310.4546.pdf\n    \"\"\"\n\n    identifier = 'word2vec'\n\n    def __init__(self, weights_file='GoogleNews-vectors-negative300.bin', random_embeddings=False, **kwargs):\n        weights_file = os.path.join(_ressources_dir, 'word2vec', weights_file)\n        super(Word2Vec, self).__init__(\n            identifier=self.identifier + ('-untrained' if random_embeddings else ''),\n            weights_file=weights_file, binary=True,\n            # standard embedding std\n            # https://github.com/pytorch/pytorch/blob/ecbf6f99e6a4e373105133b31534c9fb50f2acca/torch/nn/modules/sparse.py#L106\n            random_std=1, random_embeddings=random_embeddings, **kwargs)\n\n\nclass Glove(KeyedVectorModel):\n    \"\"\"\n    Pennington et al., 2014\n    http://www.aclweb.org/anthology/D14-1162\n    \"\"\"\n\n    identifier = 'glove'\n\n    def __init__(self, weights='glove.840B.300d.txt', random_embeddings=False, **kwargs):\n        from gensim.scripts.glove2word2vec import glove2word2vec\n        weights_file = os.path.join(_ressources_dir, 'glove', weights)\n        word2vec_weightsfile = weights_file + '.word2vec'\n        if not os.path.isfile(word2vec_weightsfile):\n            glove2word2vec(weights_file, word2vec_weightsfile)\n        super(Glove, self).__init__(\n            identifier=self.identifier + ('-untrained' if random_embeddings else ''), weights_file=word2vec_weightsfile,\n            # std from https://gist.github.com/MatthieuBizien/de26a7a2663f00ca16d8d2558815e9a6#file-fast_glove-py-L16\n            random_std=.01, random_embeddings=random_embeddings, **kwargs)\n\n\nclass RecursiveNeuralTensorNetwork(BrainModel, TaskModel):\n    \"\"\"\n    http://www.aclweb.org/anthology/D13-1170\n    \"\"\"\n\n    def __init__(self, weights='sentiment'):\n        cachepath = os.path.join(_ressources_dir, 'recursive-neural-tensor-network', weights + '.activations.csv')\n        self._cache = pd.read_csv(cachepath)\n        self._cache = self._cache[self._cache['node.type'] == 'ROOT']\n        self._cache.drop_duplicates(inplace=True)\n\n    def __call__(self, sentences):\n        result = self._cache[self._cache['sentence'].isin(sentences)\n                             | self._cache['sentence'].isin([sentence + '.' for sentence in sentences])]\n        if len(result) != 1:\n            print(sentences)\n        assert len(result) == 1\n        result = result[[column for column in result if column.startswith('activation')]]\n        return result.values\n\n\ndef _call_conditional_average(*args, extractor, average_sentence, sentence_averaging, **kwargs):\n    if average_sentence:\n        handle = extractor.register_activations_hook(sentence_averaging)\n    result = extractor(*args, **kwargs)\n    if average_sentence:\n        handle.remove()\n    return result\n\n\ndef load_model(model_name):\n    return model_pool[model_name]\n\n\nmodel_pool = {\n    SentenceLength.identifier: LazyLoad(SentenceLength),\n    WordPosition.identifier: LazyLoad(WordPosition),\n    RandomEmbedding.identifier: LazyLoad(RandomEmbedding),\n    SkipThoughts.identifier: LazyLoad(SkipThoughts),\n    SkipThoughts.identifier + '-untrained': LazyLoad(lambda: SkipThoughts(load_weights=False)),\n    LM1B.identifier: LazyLoad(LM1B),\n    LM1B.identifier + '-untrained': LazyLoad(lambda: LM1B(reset_weights=True)),\n    Word2Vec.identifier: LazyLoad(Word2Vec),\n    Word2Vec.identifier + '-untrained': LazyLoad(lambda: Word2Vec(random_embeddings=True)),\n    Glove.identifier: LazyLoad(Glove),\n    Glove.identifier + '-untrained': LazyLoad(lambda: Glove(random_embeddings=True)),\n    Transformer.identifier: LazyLoad(Transformer),\n    Transformer.identifier + '-untrained': LazyLoad(lambda: Transformer(untrained=True)),\n    ETM.identifier: LazyLoad(ETM),\n    ETM.identifier + '-untrained': LazyLoad(lambda: ETM(random_embeddings=True)),\n}\nmodel_layers = {\n    SentenceLength.identifier: SentenceLength.default_layers,\n    WordPosition.identifier: WordPosition.default_layers,\n    RandomEmbedding.identifier: RandomEmbedding.default_layers,\n    SkipThoughts.identifier: SkipThoughts.default_layers,\n    LM1B.identifier: LM1B.default_layers,\n    Word2Vec.identifier: Word2Vec.default_layers,\n    Glove.identifier: Glove.default_layers,\n    Transformer.identifier: Transformer.default_layers,\n    ETM.identifier: ETM.default_layers,\n}\n# untrained layers are the same as trained ones\nmodel_layers = {**model_layers, **{f\"{identifier}-untrained\": layers for identifier, layers in model_layers.items()}}\n\nSPIECE_UNDERLINE = u'▁'  # define directly to avoid having to import (from pytorch_transformers.tokenization_xlnet)\ntransformer_configurations = []\n\"\"\"\nEach model configuration is a dictionary with the following keys:\n- identifier: if empty, use weight_identifier)\n- weight_identifier: which pretrained weights to use\n- prefix: the model's string prefix from which to build <prefix>Config, <prefix>Model, and <prefix>Tokenizer\n    if they are not defined.\n- config_ctr: the importable class name of the model's config class\n- model_ctr: the importable class name of the model's model class\n- tokenizer_ctr: the importable class name of the model's tokenizer class\n- layers: a list of layers from which we will retrieve activations\n\"\"\"\n# bert\nfor identifier, num_layers in [\n    ('bert-base-uncased', 12),\n    ('bert-base-multilingual-cased', 12),\n    ('bert-large-uncased', 24),\n    ('bert-large-uncased-whole-word-masking', 24),\n]:\n    transformer_configurations.append(dict(\n        prefix='Bert', weight_identifier=identifier,weight_file=identifier,config_file=identifier,tokenizer_identifier='bert',\n        # https://github.com/huggingface/pytorch-pretrained-BERT/blob/78462aad6113d50063d8251e27dbaadb7f44fbf0/pytorch_pretrained_bert/modeling.py#L480\n        # output == layer_norm(fc(attn) + attn)\n        layers=('embedding',) + tuple(f'encoder.layer.{i}.output' for i in range(num_layers))\n    ))\n\n\n# openaigpt\ntransformer_configurations.append(dict(\n    prefix='OpenAIGPT', identifier='openaigpt', weight_identifier='openai-gpt', tokenizer_special_tokens=('</w>',),\n    # https://github.com/huggingface/pytorch-transformers/blob/c589862b783b94a8408b40c6dc9bf4a14b2ee391/pytorch_transformers/modeling_openai.py#L517\n    layers=('drop',) + tuple(f'encoder.h.{i}.ln_2' for i in range(12))\n))\n# gpt2\nfor identifier, num_layers in [\n    ('gpt2', 12),\n    ('gpt2-medium', 24),\n    ('gpt2-large', 36),\n    ('gpt2-xl', 48),\n    ('distilgpt2', 6),\n]:\n    transformer_configurations.append(dict(\n        prefix='GPT2', weight_identifier=identifier,weight_file=identifier,config_file=identifier, tokenizer_special_tokens=('ġ',),tokenizer_identifier='gpt2',\n        # https://github.com/huggingface/pytorch-transformers/blob/c589862b783b94a8408b40c6dc9bf4a14b2ee391/pytorch_transformers/modeling_gpt2.py#L514\n        layers=('drop',) + tuple(f'encoder.h.{i}' for i in range(num_layers))\n    ))\n\n\ncheckpoints=['/om/user/ehoseini/MyData/miniBERTa_training/miniBERTa_1b_v2/gpt2/checkpoints_4/']\nfor (identifier, num_layers), ckpnts in itertools.product([\n    ('gpt2-neox-pos_learned-1B-v2', 12,)], np.arange(2500,320000+2500,2500)):\n    identifier = f\"{identifier}-ckpnt-{ckpnts}\"\n    transformer_configurations.append(dict(\n        prefix='gpt-neox-pos-learned', tokenizer_special_tokens=('ġ',),\n        weight_identifier=identifier,\n        weight_file=f'{checkpoints[0]}/global_step{ckpnts}/pytorch_model.bin',\n        config_file=f'{checkpoints[0]}/global_step{ckpnts}/config.json'\n        , tokenizer_identifier='gpt2',\n        layers=('drop',) + tuple(f'encoder.h.{i}' for i in range(num_layers))\n    ))\n\ncheckpoints=['/om2/user/ehoseini/MyData/miniBERTa_training/miniBERTa_1b_v2/gpt2/checkpoints_1/']\nfor (identifier, num_layers), ckpnts in itertools.product([\n    ('gpt2-neox-pos_learned-1B-v2-init2', 12,)], np.arange(107500,107501,1)):\n    identifier = f\"{identifier}-ckpnt-{ckpnts}\"\n    transformer_configurations.append(dict(\n        prefix='gpt-neox-pos-learned', tokenizer_special_tokens=('ġ',),\n        weight_identifier=identifier,\n        weight_file=f'{checkpoints[0]}/global_step{ckpnts}/pytorch_model.bin',\n        config_file=f'{checkpoints[0]}/global_step{ckpnts}/config.json'\n        , tokenizer_identifier='gpt2',\n        layers=('drop',) + tuple(f'encoder.h.{i}' for i in range(num_layers))\n    ))\n\n\n# 161 checkpoints\ncheckpoints=['/om/user/ehoseini/MyData/miniBERTa_training/miniBERTa_100m_v2/gpt2/checkpoints_5/']\nfor (identifier, num_layers), ckpnts in itertools.product([\n    ('gpt2-neox-pos_learned-100M-v2', 12,)], np.arange(250,40250+100,250)):\n    identifier = f\"{identifier}-ckpnt-{ckpnts}\"\n    transformer_configurations.append(dict(\n        prefix='gpt-neox-pos-learned', tokenizer_special_tokens=('ġ',),\n        weight_identifier=identifier, weight_file=f'{checkpoints[0]}/global_step{ckpnts}/pytorch_model.bin',\n        config_file=f'{checkpoints[0]}/global_step{ckpnts}/config.json'\n        , tokenizer_identifier='gpt2',\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_albert.py#L557\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_albert.py#L335\n        layers=('drop',) + tuple(f'encoder.h.{i}' for i in range(num_layers))\n    ))\n\n# 161 checkpoints\ncheckpoints=['/om2/user/ehoseini/MyData/miniBERTa_training/miniBERTa_100m_v2/gpt2/checkpoints_8/']\nfor (identifier, num_layers), ckpnts in itertools.product([\n    ('gpt2-neox-pos_learned-100M-v2-init2', 12,)], np.arange(17500,17501,1)):\n    identifier = f\"{identifier}-ckpnt-{ckpnts}\"\n    transformer_configurations.append(dict(\n        prefix='gpt-neox-pos-learned', tokenizer_special_tokens=('ġ',),\n        weight_identifier=identifier, weight_file=f'{checkpoints[0]}/global_step{ckpnts}/pytorch_model.bin',\n        config_file=f'{checkpoints[0]}/global_step{ckpnts}/config.json'\n        , tokenizer_identifier='gpt2',\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_albert.py#L557\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_albert.py#L335\n        layers=('drop',) + tuple(f'encoder.h.{i}' for i in range(num_layers))\n    ))\n\n\n# 46 models\ncheckpoints=['/om/user/ehoseini/MyData/miniBERTa_training/miniBERTa_10m_v2/gpt2/checkpoints_6/']\nfor (identifier, num_layers), ckpnts in itertools.product([\n    ('gpt2-neox-pos_learned-10M-v2', 12,)], np.arange(250,11500+250,250)):\n    identifier = f\"{identifier}-ckpnt-{ckpnts}\"\n    transformer_configurations.append(dict(\n        prefix='gpt-neox-pos-learned', tokenizer_special_tokens=('ġ',),\n        weight_identifier=identifier, weight_file=f'{checkpoints[0]}/global_step{ckpnts}/pytorch_model.bin',\n        config_file=f'{checkpoints[0]}/global_step{ckpnts}/config.json'\n        , tokenizer_identifier='gpt2',\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_albert.py#L557\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_albert.py#L335\n        layers=('drop',) + tuple(f'encoder.h.{i}' for i in range(num_layers))\n    ))\n\ncheckpoints=['/om2/user/ehoseini/MyData/miniBERTa_training/miniBERTa_10m_v2/gpt2/checkpoints_2/']\nfor (identifier, num_layers), ckpnts in itertools.product([\n    ('gpt2-neox-pos_learned-10M-v2-init2', 12,)], np.arange(2000,2001,1)):\n    identifier = f\"{identifier}-ckpnt-{ckpnts}\"\n    transformer_configurations.append(dict(\n        prefix='gpt-neox-pos-learned', tokenizer_special_tokens=('ġ',),\n        weight_identifier=identifier, weight_file=f'{checkpoints[0]}/global_step{ckpnts}/pytorch_model.bin',\n        config_file=f'{checkpoints[0]}/global_step{ckpnts}/config.json'\n        , tokenizer_identifier='gpt2',\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_albert.py#L557\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_albert.py#L335\n        layers=('drop',) + tuple(f'encoder.h.{i}' for i in range(num_layers))\n    ))\n\ncheckpoints=['/om/user/ehoseini/MyData/miniBERTa_training/miniBERTa_1m_v2/gpt2/checkpoints_7/']\nfor (identifier, num_layers), ckpnts in itertools.product([\n    ('gpt2-neox-pos_learned-1M-v2', 12,)], np.arange(250,4000+250,250)):\n    identifier = f\"{identifier}-ckpnt-{ckpnts}\"\n    transformer_configurations.append(dict(\n        prefix='gpt-neox-pos-learned', tokenizer_special_tokens=('ġ',),\n        weight_identifier=identifier, weight_file=f'{checkpoints[0]}/global_step{ckpnts}/pytorch_model.bin',\n        config_file=f'{checkpoints[0]}/global_step{ckpnts}/config.json'\n        , tokenizer_identifier='gpt2',\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_albert.py#L557\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_albert.py#L335\n        layers=('drop',) + tuple(f'encoder.h.{i}' for i in range(num_layers))\n    ))\n\ncheckpoints=['/om2/user/ehoseini/MyData/miniBERTa_training/miniBERTa_1m_v2/gpt2/checkpoints_1/']\nfor (identifier, num_layers), ckpnts in itertools.product([\n    ('gpt2-neox-pos_learned-1M-v2-init2', 12,)], np.arange(1000,1001,1)):\n    identifier = f\"{identifier}-ckpnt-{ckpnts}\"\n    transformer_configurations.append(dict(\n        prefix='gpt-neox-pos-learned', tokenizer_special_tokens=('ġ',),\n        weight_identifier=identifier, weight_file=f'{checkpoints[0]}/global_step{ckpnts}/pytorch_model.bin',\n        config_file=f'{checkpoints[0]}/global_step{ckpnts}/config.json'\n        , tokenizer_identifier='gpt2',\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_albert.py#L557\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_albert.py#L335\n        layers=('drop',) + tuple(f'encoder.h.{i}' for i in range(num_layers))\n    ))\n\n# checkpoints from mistral training\nmistral_root_=[ '/om/user/ehoseini/MyData/mistral/caprica-gpt2-small-x81']\n#mistral_root_=[ '/Users/eghbalhosseini/MyData/mistral/caprica-gpt2-small-x81']\nfor (identifier, num_layers), ckpoint in itertools.product([\n    ('mistral-caprica-gpt2-small-x81', 12),], list(np.concatenate([np.asarray([40,400,4000]),np.arange(0,410000,10000)]))):\n    identifier = f\"{identifier}-ckpnt-{ckpoint}\"\n    transformer_configurations.append(dict(\n        prefix='mistral', tokenizer_special_tokens=('ġ',), weight_identifier=identifier, weight_file=f\"{mistral_root_[0]}/ckpt_{ckpoint}\",\n        config_file=f\"{mistral_root_[0]}/ckpt_{ckpoint}/config.json\", tokenizer_identifier='gpt2',\n        # https://github.com/huggingface/pytorch-transformers/blob/c589862b783b94a8408b40c6dc9bf4a14b2ee391/pytorch_transformers/modeling_roberta.py#L174\n        layers=('drop',) + tuple(f'encoder.h.{i}' for i in range(num_layers))\n    ))\n\nmistral_root_=[ '/om/user/ehoseini/MyData/mistral/expanse-gpt2-small-x777']\n#mistral_root_=[ '/Users/eghbalhosseini/MyData/mistral/caprica-gpt2-small-x81']\nfor (identifier, num_layers), ckpoint in itertools.product([\n    ('expanse-gpt2-small-x777', 12),], list(np.asarray([0,40,400,4000,40000,400000]))):\n    identifier = f\"{identifier}-ckpnt-{ckpoint}\"\n    transformer_configurations.append(dict(\n        prefix='mistral', tokenizer_special_tokens=('ġ',), weight_identifier=identifier, weight_file=f\"{mistral_root_[0]}/ckpt_{ckpoint}\",\n        config_file=f\"{mistral_root_[0]}/ckpt_{ckpoint}/config.json\", tokenizer_identifier='gpt2',\n        # https://github.com/huggingface/pytorch-transformers/blob/c589862b783b94a8408b40c6dc9bf4a14b2ee391/pytorch_transformers/modeling_roberta.py#L174\n        layers=('drop',) + tuple(f'encoder.h.{i}' for i in range(num_layers))\n    ))\n\nmistral_root_=[ '/om/user/ehoseini/MyData/mistral/alias-gpt2-small-x21']\n#mistral_root_=[ '/Users/eghbalhosseini/MyData/mistral/caprica-gpt2-small-x81']\nfor (identifier, num_layers), ckpoint in itertools.product([\n    ('alias-gpt2-small-x21', 12),], list(np.asarray([0,40,400,4000,40000,400000]))):\n    identifier = f\"{identifier}-ckpnt-{ckpoint}\"\n    transformer_configurations.append(dict(\n        prefix='mistral', tokenizer_special_tokens=('ġ',), weight_identifier=identifier, weight_file=f\"{mistral_root_[0]}/ckpt_{ckpoint}\",\n        config_file=f\"{mistral_root_[0]}/ckpt_{ckpoint}/config.json\", tokenizer_identifier='gpt2',\n        # https://github.com/huggingface/pytorch-transformers/blob/c589862b783b94a8408b40c6dc9bf4a14b2ee391/pytorch_transformers/modeling_roberta.py#L174\n        layers=('drop',) + tuple(f'encoder.h.{i}' for i in range(num_layers))\n    ))\n\n\nbplm_root_=[ '/rdma/vast-rdma/vast/evlab/ehoseini/MyData/bplm/miniberta_10M/gpt2//gaussian_init_1/']\n#mistral_root_=[ '/Users/eghbalhosseini/MyData/mistral/caprica-gpt2-small-x81']\nfor (identifier, num_layers), ckpoint in itertools.product([\n    ('bplm-gpt2-gauss-init-1', 12),], list(np.arange(0,59000,500))):\n    identifier = f\"{identifier}-ckpnt-{ckpoint}\"\n    transformer_configurations.append(dict(\n        prefix='mistral', tokenizer_special_tokens=('ġ',), weight_identifier=identifier, weight_file=f\"{bplm_root_[0]}/checkpoint_{ckpoint}\",\n        config_file=f\"{bplm_root_[0]}/checkpoint_{ckpoint}/config.json\", tokenizer_identifier='gpt2',\n        # https://github.com/huggingface/pytorch-transformers/blob/c589862b783b94a8408b40c6dc9bf4a14b2ee391/pytorch_transformers/modeling_roberta.py#L174\n        layers=('drop',) + tuple(f'encoder.h.{i}' for i in range(num_layers))\n    ))\n\nbplm_root_=[ '/rdma/vast-rdma/vast/evlab/ehoseini/MyData/bplm/miniberta_10M/gpt2//gaussian_init/']\n#mistral_root_=[ '/Users/eghbalhosseini/MyData/mistral/caprica-gpt2-small-x81']\nfor (identifier, num_layers), ckpoint in itertools.product([\n    ('bplm-gpt2-gauss-init', 12),], list(np.arange(0,44000,250))):\n    identifier = f\"{identifier}-ckpnt-{ckpoint}\"\n    transformer_configurations.append(dict(\n        prefix='mistral', tokenizer_special_tokens=('ġ',), weight_identifier=identifier, weight_file=f\"{bplm_root_[0]}/checkpoint_{ckpoint}\",\n        config_file=f\"{bplm_root_[0]}/checkpoint_{ckpoint}/config.json\", tokenizer_identifier='gpt2',\n        # https://github.com/huggingface/pytorch-transformers/blob/c589862b783b94a8408b40c6dc9bf4a14b2ee391/pytorch_transformers/modeling_roberta.py#L174\n        layers=('drop',) + tuple(f'encoder.h.{i}' for i in range(num_layers))\n    ))\n\nbplm_root_=[ '/rdma/vast-rdma/vast/evlab/ehoseini/MyData/bplm/miniberta_10M/gpt2//hf_init/']\n#mistral_root_=[ '/Users/eghbalhosseini/MyData/mistral/caprica-gpt2-small-x81']\nfor (identifier, num_layers), ckpoint in itertools.product([\n    ('bplm-gpt2-hf-init', 12),], list(np.arange(0,44000,250))):\n    identifier = f\"{identifier}-ckpnt-{ckpoint}\"\n    transformer_configurations.append(dict(\n        prefix='mistral', tokenizer_special_tokens=('ġ',), weight_identifier=identifier, weight_file=f\"{bplm_root_[0]}/checkpoint_{ckpoint}\",\n        config_file=f\"{bplm_root_[0]}/checkpoint_{ckpoint}/config.json\", tokenizer_identifier='gpt2',\n        # https://github.com/huggingface/pytorch-transformers/blob/c589862b783b94a8408b40c6dc9bf4a14b2ee391/pytorch_transformers/modeling_roberta.py#L174\n        layers=('drop',) + tuple(f'encoder.h.{i}' for i in range(num_layers))\n    ))\n\n\nnyu_root_=[ '/om/user/ehoseini/MyData/nyu-roberta/']\nfor identifier, num_layers in [\n    ('nyu-mll/roberta-base-1B-1', 12),\n    ('nyu-mll/roberta-base-1B-2', 12),\n    ('nyu-mll/roberta-base-1B-3', 12),\n    ('nyu-mll/roberta-base-100M-1', 12),\n    ('nyu-mll/roberta-base-100M-2', 12),\n    ('nyu-mll/roberta-base-100M-3', 12),\n    ('nyu-mll/roberta-base-10M-1', 12),\n    ('nyu-mll/roberta-base-10M-2', 12),\n    ('nyu-mll/roberta-base-10M-3', 12),\n    ('nyu-mll/roberta-med-small-1M-1', 6),\n    ('nyu-mll/roberta-med-small-1M-2', 6),\n    ('nyu-mll/roberta-med-small-1M-3', 6)\n]:\n    transformer_configurations.append(dict(\n        prefix='nyu-mll', tokenizer_special_tokens=('ġ',), weight_identifier=identifier,weight_file=f\"{nyu_root_[0]}/{identifier}\",\n        config_file=f\"{nyu_root_[0]}/{identifier}/config.json\",tokenizer_identifier='roberta-base',\n        # https://github.com/huggingface/pytorch-transformers/blob/c589862b783b94a8408b40c6dc9bf4a14b2ee391/pytorch_transformers/modeling_roberta.py#L174\n        layers=('embedding',) + tuple(f'encoder.layer.{i}' for i in range(num_layers))\n    ))\n\n\n# transformer xl\ntransformer_configurations.append(dict(\n    prefix='TransfoXL', weight_identifier='transfo-xl-wt103',\n    # https://github.com/huggingface/pytorch-transformers/blob/c589862b783b94a8408b40c6dc9bf4a14b2ee391/pytorch_transformers/modeling_transfo_xl.py#L1161\n    layers=('drop',) + tuple(f'encoder.layers.{i}' for i in range(18))\n))\n# xlnet\nfor identifier, num_layers in [\n    ('xlnet-base-cased', 12),\n    ('xlnet-large-cased', 24),\n]:\n    transformer_configurations.append(dict(\n        prefix='XLNet', tokenizer_special_tokens=(SPIECE_UNDERLINE,), weight_identifier=identifier,weight_file=identifier,config_file=identifier,tokenizer_identifier='XLNet',\n        # https://github.com/huggingface/pytorch-transformers/blob/c589862b783b94a8408b40c6dc9bf4a14b2ee391/pytorch_transformers/modeling_xlnet.py#L962\n        layers=('drop',) + tuple(f'encoder.layer.{i}' for i in range(num_layers))\n    ))\n# xlm\nfor identifier, num_layers in [\n    ('xlm-mlm-en-2048', 12),\n    ('xlm-mlm-enfr-1024', 6),\n    ('xlm-mlm-xnli15-1024', 12),\n    ('xlm-clm-enfr-1024', 6),\n    ('xlm-mlm-100-1280', 16),\n]:\n    transformer_configurations.append(dict(\n        prefix='XLM', tokenizer_special_tokens=('</w>',), weight_identifier=identifier,weight_file=identifier,config_file=identifier,tokenizer_identifier='XLM',\n        # https://github.com/huggingface/pytorch-transformers/blob/c589862b783b94a8408b40c6dc9bf4a14b2ee391/pytorch_transformers/modeling_xlm.py#L638\n        layers=('dropout',) + tuple(f'encoder.layer_norm2.{i}' for i in range(num_layers))\n    ))\n# roberta\nfor identifier, num_layers in [\n    ('roberta-base', 12),\n    ('roberta-large', 24),\n    ('distilroberta-base', 6),\n]:\n    transformer_configurations.append(dict(\n        prefix='Roberta', tokenizer_special_tokens=('ġ',), weight_identifier=identifier,weight_file=identifier,config_file=identifier,tokenizer_identifier='Roberta',\n        # https://github.com/huggingface/pytorch-transformers/blob/c589862b783b94a8408b40c6dc9bf4a14b2ee391/pytorch_transformers/modeling_roberta.py#L174\n        layers=('embedding',) + tuple(f'encoder.layer.{i}' for i in range(num_layers))\n    ))\n\n\n# distilbert\nfor identifier, num_layers in [\n    ('distilbert-base-uncased', 6),\n]:\n    transformer_configurations.append(dict(\n        prefix='DistilBert', tokenizer_special_tokens=('ġ',), weight_identifier=identifier,weight_file=identifier,config_file=identifier,\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_distilbert.py#L482\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_distilbert.py#L258\n        layers=('embeddings',) + tuple(f'transformer.layer.{i}' for i in range(num_layers))\n    ))\n# ctrl\ntransformer_configurations.append(dict(\n    prefix='CTRL', tokenizer_special_tokens=('ġ',), weight_identifier='ctrl',weight_file=identifier,config_file=identifier,\n    # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_ctrl.py#L388\n    # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_ctrl.py#L408\n    layers=('w',) + tuple(f'h.{i}' for i in range(48))\n))\n# albert\nfor (identifier, num_layers), version in itertools.product([\n    ('albert-base', 12),\n    ('albert-large', 24),\n    ('albert-xlarge', 24),\n    ('albert-xxlarge', 12),\n], [1, 2]):\n    identifier = f\"{identifier}-v{version}\"\n    transformer_configurations.append(dict(\n        prefix='Albert', tokenizer_special_tokens=('ġ',), weight_identifier=identifier,weight_file=identifier,config_file=identifier,tokenizer_identifier='Albert',\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_albert.py#L557\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_albert.py#L335\n        layers=('embeddings',) + tuple(f'encoder.albert_layer_groups.{i}' for i in range(num_layers))\n    ))\n\n\n# t5\nclass _T5Wrapper:\n    def __init__(self, model):\n        self._model = model\n\n    def __call__(self, input_ids, **kwargs):\n        # the decoder_input_ids are not right, but we only retrieve encoder features anyway\n        return self._model(encoder_input_ids=input_ids, decoder_input_ids=input_ids, **kwargs)\n\n    def __getattr__(self, item):  # forward attribute retrieval\n        if item in ['_model', 'to']:\n            return super(_T5Wrapper, self).__getattr__(item)\n        return getattr(self._model, item)\n\n    def to(self, *args, **kwargs):\n        self._model = self._model.to(*args, **kwargs)\n        return self\n\n\nfor identifier, num_layers in [\n    ('t5-small', 6),\n    ('t5-base', 12),\n    ('t5-large', 24),\n    ('t5-3b', 24),\n    ('t5-11b', 24),\n]:\n    transformer_configurations.append(dict(\n        prefix='T5', tokenizer_special_tokens=('ġ',), weight_identifier=identifier,weight_file=identifier,config_file=identifier,\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_t5.py#L773\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_t5.py#L605\n        layers=('shared',) + tuple(f'encoder.block.{i}' for i in range(num_layers)),\n        model_wrapper=_T5Wrapper,\n    ))\n# xlm-roberta\nfor identifier, num_layers in [\n    ('xlm-roberta-base', 12),\n    ('xlm-roberta-large', 24),\n]:\n    transformer_configurations.append(dict(\n        prefix='XLMRoberta', tokenizer_special_tokens=('ġ',), weight_identifier=identifier,weight_file=identifier,config_file=identifier,\n        # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_xlm_roberta.py#L119\n        layers=('embedding',) + tuple(f'encoder.layer.{i}' for i in range(num_layers))\n    ))\n\nfor condition in ['trained','untrained','untrained_hf','permuted','untrained-1','untrained-2','untrained-3','untrained-4','untrained-5','untrained-6','untrained-7','untrained-std-1','untrained-std-2','untrained-mu-1','untrained-mu-2']:\n#for untrained in False, True:\n    for configuration in transformer_configurations:\n        configuration = copy.deepcopy(configuration)\n        # either use the defined identifier or the weights used\n        identifier = configuration.get('identifier', configuration['weight_identifier'])\n        configuration['trained'] = True\n        configuration['permuted'] = False\n        prefix = configuration.get('identifier', configuration['prefix'])\n\n        if 'untrained' in condition:\n            identifier += f'-{condition}'\n            configuration['trained'] = False\n            configuration['permuted'] = False\n            configuration['untrained_type'] = condition\n        #if condition == 'permuted':\n        if 'permuted' in condition:\n            identifier += f'-{condition}'\n            configuration['trained'] = True\n            configuration['permuted'] = True\n            configuration['permute_type'] = condition\n\n        if prefix == 'nyu-mll':\n            configuration['config_ctr'] = configuration.get('config_ctr', 'AutoConfig')\n            configuration['model_ctr'] = configuration.get('model_ctr', 'AutoModelForMaskedLM')\n            configuration['tokenizer_ctr'] = configuration.get('tokenizer_ctr', 'AutoTokenizer')\n            configuration['module_ctr'] = 'transformers'\n        elif prefix == 'stanford-crfm' or prefix == 'mistral':\n            configuration['config_ctr'] = configuration.get('config_ctr', 'AutoConfig')\n            configuration['model_ctr'] = configuration.get('model_ctr', 'AutoModelForCausalLM')\n            configuration['tokenizer_ctr'] = configuration.get('tokenizer_ctr', 'AutoTokenizer')\n            configuration['module_ctr'] = 'transformers'\n        elif prefix == 'gpt-neox-pos-learned':\n            configuration['config_ctr'] = configuration.get('config_ctr', 'GPTNeoXPosLearnedConfig')\n            configuration['model_ctr'] = configuration.get('model_ctr', 'GPTNeoXPosLearnedModel')\n            configuration['tokenizer_ctr'] = configuration.get('tokenizer_ctr', 'GPT2Tokenizer')\n            configuration['module_ctr'] = 'neural_nlp.models.gpt_neox_model'\n        else:\n        # either use the defined values for config, model and tokenizer or build from prefix\n            configuration['config_ctr'] = configuration.get('config_ctr', configuration['prefix'] + 'Config')\n            configuration['model_ctr'] = configuration.get('model_ctr', configuration['prefix'] + 'Model')\n            configuration['tokenizer_ctr'] = configuration.get('tokenizer_ctr', configuration['prefix'] + 'Tokenizer')\n            configuration['module_ctr'] = 'transformers'\n\n        def model_instantiation(identifier=identifier, configuration=frozenset(configuration.items())):\n            configuration = dict(configuration)  # restore from frozen\n            module = import_module(configuration['module_ctr'])\n            config_ctr = getattr(module, configuration['config_ctr'])\n            config = config_ctr.from_pretrained(configuration['config_file'])\n            model_ctr = getattr(module, configuration['model_ctr'])\n            tokenizer_ctr = getattr(module, configuration['tokenizer_ctr'])\n            # Load pre-trained model tokenizer (vocabulary) and model\n            tokenizer = tokenizer_ctr.from_pretrained(configuration['tokenizer_identifier'])\n\n            state_dict = None\n            config.output_hidden_states = True\n            if not configuration.get('trained', True):  # if untrained\n                # load standard model constructor: this will only create modules and initialize them for training\n                if \"GPTNeoXPosLearned\" in configuration['config_ctr']:\n\n                    model = model_ctr(config=config)\n                    if configuration['untrained_type']=='untrained_hf':\n                        state_dict = model.state_dict()\n                    elif configuration['untrained_type']=='untrained':\n                        print('initializing model manually\\n')\n                        state_dict = initialize_gpt_neox_weights(model,permute=False)\n\n                elif \"AutoConfig\" in configuration['config_ctr']:\n                    print('initializing model manually\\n')\n                    model = model_ctr.from_config(config=config)\n                    if configuration['untrained_type'] == 'untrained-1':\n                        state_dict = initialize_gpt2_weights(model, permute=False,\n                                                             valid_keys=['attn.c_attn.weight','attn.c_attn.bias','attn.c_proj','ln_2','mlp.c_fc','mlp.c_proj','wte','wpe','lm_head']) # remove ln_1 shuffling\n                    elif configuration['untrained_type'] == 'untrained-2':\n                        state_dict = initialize_gpt2_weights(model, permute=False,\n                                                             valid_keys=['attn.c_proj','ln_1','ln_2','mlp.c_fc','mlp.c_proj','wte','wpe','lm_head']) # remove c_attn.weight\n                    elif configuration['untrained_type'] == 'untrained-3':\n                        state_dict = initialize_gpt2_weights(model, permute=False,\n                                                             valid_keys=['attn.c_attn.weight','attn.c_attn.bias','ln_1','ln_2','mlp.c_fc','mlp.c_proj','wte','wpe','lm_head']) # remove attn.c_proj\n                    elif configuration['untrained_type'] == 'untrained-4':\n                        state_dict = initialize_gpt2_weights(model, permute=False,\n                                                             valid_keys=['attn.c_attn.weight','attn.c_attn.bias','attn.c_proj','ln_1','mlp.c_fc','mlp.c_proj','wte','wpe','lm_head']) # remove ln_2\n                    elif configuration['untrained_type'] == 'untrained-5':\n                        state_dict = initialize_gpt2_weights(model, permute=False,\n                                                             valid_keys=['attn.c_attn.weight','attn.c_attn.bias','attn.c_proj','ln_1','ln_2','mlp.c_proj','wte','wpe','lm_head']) # remove mlp c_fc\n                    elif configuration['untrained_type'] == 'untrained-6':\n                        state_dict = initialize_gpt2_weights(model, permute=False,\n                                                             valid_keys=['attn.c_attn.weight','attn.c_attn.bias','attn.c_proj','ln_1','ln_2','mlp.c_fc','wte','wpe','lm_head']) # remove mlp_proj\n                    elif configuration['untrained_type'] == 'untrained-7':\n                        state_dict = initialize_gpt2_weights(model, permute=False,\n                                                             valid_keys=['None'])  # remove all\n                    elif configuration['untrained_type'] == 'untrained-std-1':\n                        state_dict = initialize_gpt2_weights(model, permute=False,sigma=0.2)  # remove all\n                    elif configuration['untrained_type'] == 'untrained-std-2':\n                        state_dict = initialize_gpt2_weights(model, permute=False,sigma=2)  # remove all\n                    elif configuration['untrained_type'] == 'untrained-mu-1':\n                        state_dict = initialize_gpt2_weights(model, permute=False,mu=.5, sigma=0.2)  # remove all\n                    elif configuration['untrained_type'] == 'untrained-mu-2':\n                        state_dict = initialize_gpt2_weights(model, permute=False,mu=1, sigma=0.2)  # remove all\n\n\n                    elif configuration['untrained_type'] == 'untrained':\n                        state_dict = initialize_gpt2_weights(model,permute=False)\n                    elif configuration['untrained_type'] == 'untrained_hf':\n                        state_dict = model.state_dict()\n                else:\n                    #model = model_ctr.from_config(config=config)\n                    model = model_ctr(config=config)\n                    state_dict = model.state_dict()  # capture initial random weights and force load them later\n            if configuration['prefix'] == 'gpt-neox-pos-learned':\n                config.output_hidden_states = True\n                model = model_ctr.from_pretrained(configuration['weight_file'], config=config,state_dict=state_dict)\n                if configuration['permuted']:\n                    state_dict = initialize_gpt_neox_weights(model,permute=True)\n                    model = model_ctr.from_pretrained(configuration['weight_file'], config=config,\n                                                      state_dict=state_dict)\n\n            elif configuration['prefix']=='mistral':\n\n                config.output_hidden_states = True\n                model = model_ctr.from_pretrained(configuration['weight_file'], config=config, state_dict=state_dict)\n                if configuration['permuted']:\n                    if configuration['permute_type'] == 'permuted-1':\n                        state_dict = initialize_gpt2_weights(model,permute=True,valid_keys=['attn.c_attn.bias','attn.c_proj','ln','mlp','wte','wpe','lm_head'])\n                    elif configuration['permute_type'] == 'permuted-2':\n                        state_dict = initialize_gpt2_weights(model, permute=True,valid_keys=['attn.c_attn.weight','attn.c_attn.bias','attn.c_proj','ln','mlp','wte','wpe','lm_head'])\n                    elif configuration['permute_type'] == 'permuted-3':\n                        state_dict = initialize_gpt2_weights(model, permute=True,valid_keys=['attn.c_attn.weight','attn.c_attn.bias','attn.c_proj','ln','mlp','wte','wpe','lm_head'])\n                    elif configuration['permute_type'] == 'permuted':\n                        state_dict = initialize_gpt2_weights(model, permute=True,valid_keys=['attn.c_attn.weight','attn.c_attn.bias','attn.c_proj','ln','mlp','wte','wpe','lm_head'])\n                    model = model_ctr.from_pretrained(configuration['weight_file'], config=config,\n                                                      state_dict=state_dict)\n\n\n            elif configuration['prefix']=='nyu-mll':\n\n                config.output_hidden_states = True\n                model = model_ctr.from_pretrained(configuration['weight_file'], config=config, state_dict=state_dict)\n                if configuration['permuted']:\n                    state_dict = initialize_gpt2_weights(model,permute=True)\n                    model = model_ctr.from_pretrained(configuration['weight_file'], config=config,\n                                                      state_dict=state_dict)\n            else:\n                model = model_ctr.from_pretrained(configuration['weight_file'], output_hidden_states=True,state_dict=state_dict)\n\n            model_wrapper = configuration.get('model_wrapper', None)\n            if model_wrapper:\n                model = model_wrapper(model)\n            transformer = _PytorchTransformerWrapper(\n                identifier=identifier,\n                tokenizer=tokenizer, tokenizer_special_tokens=configuration.get('tokenizer_special_tokens', ()),\n                model=model, layers=configuration['layers'],\n                sentence_average=word_last)\n            return transformer\n\n\n        model_pool[identifier] = LazyLoad(model_instantiation)\n        model_layers[identifier] = list(configuration['layers'])\n
Subsystem: com.intellij.openapi.diff.impl.patch.CharsetEP
<+>UTF-8
===================================================================
diff --git a/neural_nlp/models/implementations.py b/neural_nlp/models/implementations.py
--- a/neural_nlp/models/implementations.py	
+++ b/neural_nlp/models/implementations.py	
@@ -1465,7 +1465,7 @@
     ))
 # ctrl
 transformer_configurations.append(dict(
-    prefix='CTRL', tokenizer_special_tokens=('ġ',), weight_identifier='ctrl',weight_file=identifier,config_file=identifier,
+    prefix='CTRL', tokenizer_special_tokens=('ġ',), weight_identifier='ctrl',weight_file=identifier,config_file=identifier,tokenizer_identifier='ctrl',
     # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_ctrl.py#L388
     # https://github.com/huggingface/transformers/blob/80faf22b4ac194061a08fde09ad8b202118c151e/src/transformers/modeling_ctrl.py#L408
     layers=('w',) + tuple(f'h.{i}' for i in range(48))
